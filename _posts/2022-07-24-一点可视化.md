---
title: 一点可视化
tag: visualize
---

# 将图片变成patch

使用einops能够很简单地实现图片变成patch的功能。

这里为了简单地可视化，将每个patch pad一定的值以示区分。

```python
import einops
import numpy as np
import mmcv
import cv2
from PIL import Image

img = mmcv.imread(r'C:\Users\czh\Desktop\test.png', channel_order='rgb')
h, w, c = img.shape
p = 64 # patch_size
pad = 2
img2 = cv2.resize(img, (w // p * p, h // p * p))
img_patches = einops.rearrange(img2, '(h h1) (w w1) c -> (h w) c h1 w1', h1=p, w1=p)
img_patches = torch.tensor(img_patches)

padded_pathes = F.pad(img_patches, [pad]*4, mode='constant', value=255)
img_padded = einops.rearrange(padded_pathes, '(h w) c h1 w1 -> (h h1) (w w1) c', h1=p+pad*2, w1=p+pad*2, h=img2.shape[0]//p)

Image.fromarray(img_padded.numpy())
```

原始图片：

![test2](https://raw.githubusercontent.com/294coder/blog_img_bed/main/imgs/test2.png)

变成patch之后的图片（可视化）：

![output](https://raw.githubusercontent.com/294coder/blog_img_bed/main/imgs/output.png)

> 在ViT中，是直接使用`nn.Conv2d`进行卷积操作，上面只是一个可视化。

## 其他的实现方法

当然也可以利用`F.unfold`和`F.fold`实现，只是这样就会很麻烦，需要很多次reshape和permute以保证正确的维度。

具体地，比如将feature map变成token也是相同的做法，可以参考[MobileViT v2](https://github.com/apple/ml-cvnets/blob/main/cvnets/modules/mobilevit_block.py)的代码。

```python
img_tensor = torch.from_numpy(img2)[None, ...].permute(0, -1, 1, 2).float() # [B, C, H, W]
B, C, H, W = img_tensor.shape  # 这里因为只有一张图片，所以B = 1

# unfold成patch，只不过将C和patch_size合成了一个维度
img_patches = F.unfold(img_tensor, kernel_size=(p, p), stride=(p, p)) # [B, p*p*C, H/p*W/p] or [B, C*patch_size, num_patch]
num_patch = img_patches.shape[-1]
img_patches = img_patches.reshape(B, C, p, p, num_patch).permute(0, -1, 1, 2, 3).reshape(B*num_patch, C, p, p)

# padding
padded_pathes = F.pad(img_patches, [pad]*4, mode='constant', value=255) # [B*num_patch, C, p+2*pad, p+2*pad]
pad_h = pad_w = p + 2 * pad
# 保证维度正确
padded_pathes = padded_pathes.reshape(B, num_patch, C, pad_h, pad_w).permute(0, 2, -2, -1, 1).reshape(B, C*pad_h*pad_w, num_patch)

img_padded_h = H // p * (pad * 2 + p)
img_padded_w = W // p * (pad * 2 + p)
# 再fold成图像的形状
# 需要注意的是，fold的时候需要计算一下输出的大小output_size
img_padded = F.fold(padded_pathes, output_size=(img_padded_h, img_padded_w), kernel_size=(pad_h, pad_w), stride=(pad_h, pad_w))[0] # get 1 img
img_padded = img_padded.permute(1, 2, 0).to(torch.uint8)

Image.fromarray(img_padded.numpy())

```

还有利用`F.conv2d`和`F.pixel_shuffle`做的

```python
weights = torch.eye(p * p, dtype=torch.float)
weights = weights.reshape(p * p, 1, p, p).repeat(C, 1, 1, 1)  # [p*p*C, 1, p, p]
# groups必须被in_channel和out_channel同时整除

# 那么在做conv时，kernel[1, p, p]分别就是[1, 0, 0, ..., 0] [0, 1, 0, ..., 0]
#                                      |0, 0, 0, ..., 0| |0, 0, 0, ..., 0|
#                                      |.  .  .  ...  .| |.  .  .  ...  .| ...
#                                      [0, 0, 0, ..., 0] [0, 0, 0, ..., 0]

# 也就是说做完卷积之后得到的张量的每一维都是对应kernel的1所在的位置乘以（等于）对应位置
# img的像素值，这样再做pixel_shuffle就能返回到原图

# 实际上就等价于img_patches = F.pixel_unshuffle(img_tensor, downscale_factor=p)
img_patches = F.conv2d(img_tensor, weights, bias=None, stride=(p, p), groups=C)  # [B, p*p*C, H//p, W//p]
num_patch = H // p * W // p
img_patches = img_patches.reshape(B, C, p**2, H//p, W//p).permute(0, -2, -1, 1, 2).reshape(B*num_patch, C, p, p)

padded_pathes = F.pad(img_patches, [pad]*4, mode='constant', value=255) # [B*num_patch, C, p+2*pad, p+2*pad]
pad_h = pad_w = p + 2 * pad
padded_pathes = padded_pathes.reshape(B, num_patch, C, pad_h, pad_w).permute(0, 2, -2, -1, 1).reshape(B, C*pad_h*pad_w, H//p, W//p)

img_padded = F.pixel_shuffle(padded_pathes, upscale_factor=pad_h)[0].permute(-2, -1, 0).to(torch.uint8)


Image.fromarray(img_padded.numpy())

```

也能达到相同的效果。
