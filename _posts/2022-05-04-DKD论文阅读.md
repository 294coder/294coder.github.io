paper: [https://arxiv.org/abs/2203.08679](https://arxiv.org/abs/2203.08679)
代码仓库：[https://github.com/megvii-research/mdistiller/blob/master/mdistiller/distillers/DKD.py](https://github.com/megvii-research/mdistiller/blob/master/mdistiller/distillers/DKD.py)
# ![image.png](https://cdn.nlark.com/yuque/0/2022/png/12360969/1649851657198-2dd63f40-3427-4e18-b4a5-2d3d1e3a37d9.png#clientId=u12d9797f-377e-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=520&id=ubb4c7c8f&margin=%5Bobject%20Object%5D&name=image.png&originHeight=815&originWidth=808&originalType=binary&ratio=1&rotation=0&showTitle=false&size=167549&status=done&style=none&taskId=u7aeff727-80dc-4261-b8f5-89082abe862&title=&width=515.4000244140625)
# Intuition
论文中说现在SoTA的蒸馏方法是对中间层的特征进行蒸馏，因为其含有高级的语义信息（Semantic feature），但是直接对网络输出的logits进行蒸馏的效果却不是很好。这很违反直觉，因为按照道理，logits应该含有更高的语义信息才对。
# 解耦（decouple）
为此作者将标准的知识蒸馏的loss解耦为下面公式
$KD=KL(\bm b^T\| \bm b^S)+KL(\hat {\bm p}^T\| \hat {\bm p}^S)$
第一项称之为Target Class Knowledge Distillation，第二项称之为Non-target Class Knowledge Distillation。
> Firstly, we divide a classification prediction into two levels: (1) a binary prediction for the target class and _all the non-target classes _and (2) a multi-category prediction for _each non-target class_.

## 推导
下面我们来一步一步地推导。
首先，对于经典的KD方法，输出概率为$p_i=\cfrac{\exp(z_i)}{\sum_{j=1}^C \exp(z_j)}$，$C$是总共的类别数，$z$是logits。
我们将其解耦为ground truth的部分$p_t$和不为ground truth的部分$p_{\backslash t}$（对于论文中的target和non-target）：
$p_t=\cfrac{\exp{z_t}}{\sum_{j=1}^C \exp(z_j)},\ p_{\backslash{t}}=\cfrac{\sum_{k=1, k\neq t}^C \exp(z_k)}{\sum_{j=1}^C\exp(z_j)}$

定义一个$\hat{\bm p}=[\hat p_1,\cdots, \hat p_{t-1},\hat p_{t+1}, \cdots, \hat p_C]\in \R^{1\times C-1}$表示non-target的部分（注意现在就不是多个类别了）
$\hat p_i=\cfrac{\exp(z_i)}{\sum_{j=1,j\neq t}^C\exp(z_j)}$
至此，我们就可以将经典的KD loss改写为下面式子：
$$KD=KL(\bm p^{\mathcal T}\|\bm p^{\mathcal S})
=\underbrace{p_t^{\mathcal T}\log(\cfrac{p_t^{\mathcal T}}{p_t^{\mathcal S}}) }_{\text{target}}+\underbrace{\sum_{i=1,i\neq t}^C p_i^{\mathcal T}\log(\cfrac{p_i^{\mathcal T}}{p_i^{\mathcal S}})}_{\text{non-target}}

\\=p_t^{\mathcal T}\log(\cfrac{p_t^{\mathcal T}}{p_t^{\mathcal S}})+p_{\backslash t}^{\mathcal T}\sum_{i=1,i\neq t}^C \hat p_i^{\mathcal T}\big(\log(\cfrac{\hat p_i^{\mathcal T}}{\hat p_i^{\mathcal S}})+\log(\cfrac{\hat p_{\backslash t}^{\mathcal T}}{\hat p_{\backslash t}^{\mathcal S}})\big)$
然后就可以写为这样
$KD=\underbrace{p_t^{\mathcal T}\log(\cfrac{p_t^{\mathcal T}}{p_t^{\mathcal S}})+\hat p_{\backslash t}^{\mathcal T}\log(\cfrac{\hat p_{\backslash t}^{\mathcal T}}{\hat p_{\backslash t}^{\mathcal S}})}_{KL(\bm b^{\mathcal T}\|\bm b^{\mathcal S})}+
p_{\backslash t}^{\mathcal T}\underbrace{\sum_{i=1,i\neq t}^C \hat p_i^{\mathcal T}\log(\cfrac{\hat p_i^{\mathcal T}}{\hat p_i^{\mathcal S}})}_{KL(\hat {\bm p}^T\| \hat {\bm p}^S)}\\
=KL(\bm b^T\| \bm b^S)+(1-p_{t})^{\mathcal T}KL(\hat {\bm p}^T\| \hat {\bm p}^S)$$
第一项就是TCKD，第二项就是NCKD。
# 表现
作者经过实验认为NCKD在KD中的贡献很大，在一些干净的数据集上可以和经典KD媲美甚至好于KD。
![image.png](https://cdn.nlark.com/yuque/0/2022/png/12360969/1649854716371-077d6d51-a815-4b25-8ff8-bdd5d2be453c.png#clientId=u12d9797f-377e-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=437&id=udc47d65f&margin=%5Bobject%20Object%5D&name=image.png&originHeight=618&originWidth=632&originalType=binary&ratio=1&rotation=0&showTitle=false&size=90272&status=done&style=none&taskId=ua4151a69-93ad-4189-a529-cbc52dcd0cf&title=&width=446.6000061035156)
当然TCKD也不是没有用，在一些很难的任务上可以发挥很大的作用。

![image.png](https://cdn.nlark.com/yuque/0/2022/png/12360969/1649854946526-cf7faae4-dca2-44ca-af28-a9fe9220c73f.png#clientId=u12d9797f-377e-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=421&id=u7f7e0797&margin=%5Bobject%20Object%5D&name=image.png&originHeight=564&originWidth=607&originalType=binary&ratio=1&rotation=0&showTitle=false&size=93452&status=done&style=none&taskId=uc474b9da-a939-4588-be76-66cbb0fc3ba&title=&width=452.6000061035156)
![image.png](https://cdn.nlark.com/yuque/0/2022/png/12360969/1649854966610-66d9a861-9e3d-45c1-a5ad-020a19c63844.png#clientId=u12d9797f-377e-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=133&id=ud37bd69e&margin=%5Bobject%20Object%5D&name=image.png&originHeight=166&originWidth=603&originalType=binary&ratio=1&rotation=0&showTitle=false&size=25450&status=done&style=none&taskId=ufeab937d-8158-4025-8a32-25f932eff70&title=&width=482.4)
下面是与不同的知识蒸馏的方法的对比：
![image.png](https://cdn.nlark.com/yuque/0/2022/png/12360969/1649854990270-abc8a2eb-0e7c-4896-a11a-dbd1ec29c7b0.png#clientId=u12d9797f-377e-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=454&id=u919c6f20&margin=%5Bobject%20Object%5D&name=image.png&originHeight=568&originWidth=1692&originalType=binary&ratio=1&rotation=0&showTitle=false&size=169684&status=done&style=none&taskId=u8a3d02cb-144f-443c-891f-f58c259804c&title=&width=1353.6)
# 代码
```python
def dkd_loss(logits_student, logits_teacher, target, alpha, beta, temperature):
    gt_mask = _get_gt_mask(logits_student, target)
    other_mask = _get_other_mask(logits_student, target)
    
    pred_student = F.softmax(logits_student / temperature, dim=1)
    pred_teacher = F.softmax(logits_teacher / temperature, dim=1)
    pred_student = cat_mask(pred_student, gt_mask, other_mask)
    pred_teacher = cat_mask(pred_teacher, gt_mask, other_mask)
    
    log_pred_student = torch.log(pred_student) # 使用teacher去指导student，需要将student log一下
    # 对应论文中的KL(\bm b^T\| \bm b^S)
    tckd_loss = (
        F.kl_div(log_pred_student, pred_teacher, size_average=False)
        * (temperature**2)
        / target.shape[0]
    )
    # 对应论文中的KL(\hat {\bm p}^T\| \hat {\bm p}^S)
    pred_teacher_part2 = F.softmax(
        logits_teacher / temperature - 1000.0 * gt_mask, dim=1
    )
    log_pred_student_part2 = F.log_softmax(
        logits_student / temperature - 1000.0 * gt_mask, dim=1
    )
    nckd_loss = (
        F.kl_div(log_pred_student_part2, pred_teacher_part2, size_average=False)
        * (temperature**2)
        / target.shape[0]
    )
    return alpha * tckd_loss + beta * nckd_loss


def _get_gt_mask(logits, target):
    target = target.reshape(-1)
    mask = torch.zeros_like(logits).scatter_(1, target.unsqueeze(1), 1).bool()
    return mask


def _get_other_mask(logits, target):
    target = target.reshape(-1)
    mask = torch.ones_like(logits).scatter_(1, target.unsqueeze(1), 0).bool()
    return mask


def cat_mask(t, mask1, mask2):
    '''
    将target变成binary的形式
    '''
    t1 = (t * mask1).sum(dim=1, keepdims=True) # target class
    t2 = (t * mask2).sum(1, keepdims=True)# non-target class
    rt = torch.cat([t1, t2], dim=1) # concatenate
    return rt
```
测试
```python
logits_s = torch.tensor([[.2, .3, .5, .9], [1.1, .3, .02, .9]])
logits_t = torch.tensor([[.4, .1, .5, 1.3], [0.9, .1, .02, 1.2]])
target = torch.tensor([3, 3], dtype=torch.int64)
dkd_loss(logits_s, logits_t, target, alpha=0.1, beta=0.9, temperature=1)
```
输出：
```
tensor(0.0092)
```

